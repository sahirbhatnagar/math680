\documentclass[11pt,letter]{article}    
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amssymb} %math symbols
\usepackage{amsmath,amsthm,amssymb,bbm} %math stuff
\usepackage{mathrsfs}
%\usepackage[margin=0.1in]{geometry}
%\usepackage{fullpage}
\usepackage{comment}
\usepackage{ctable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{comment}
\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
\bibliographystyle{plainnat}
\usepackage{placeins}
\usepackage{epstopdf}
\usepackage{setspace} %Spacing
\usepackage{graphicx,graphics}
\usepackage{booktabs,tabularx}
\usepackage{enumerate}
\usepackage{makecell}
\usepackage{xfrac}
\usepackage[latin1]{inputenc}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{listings} 
\lstloadlanguages{R} 
\lstset{language=R,basicstyle=\smaller[2],commentstyle=\rmfamily\smaller, 
  showstringspaces=false,% 
  xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1} 
\lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside lstlistings (Scode) 
%\usepackage[usenames,dvipsnames]{pstricks}
%\usepackage{epsfig}
%\usepackage{pst-grad} % For gradients
%\usepackage{pst-plot} % For axes

\usepackage{color, colortbl, xcolor}
\definecolor{Gray}{gray}{0.9}

\newcommand {\dsum}{\displaystyle \sum}
\newcommand {\expp}[1]{\exp \left\lbrace {#1}\right\rbrace }
\newcommand {\dprod}{\displaystyle \prod} 
\newcommand {\E}{\mathbb{E}} 
\newcommand {\bs}{\boldsymbol} 
\newcommand{\dpart}[2]{\frac{\partial #1}{\partial #2}}
\newcommand {\tlde}[1] {\underset{\widetilde{}}{#1}}
\newcommand{\inprob}{\xrightarrow p}
\newcommand{\indist}{\xrightarrow d}
\newcommand{\xn}{X_1,\ldots,X_n}
\newcommand{\sumn}{\sum\limits_{i=1}^{n}}
\newcommand{\sumj}{\sum\limits_{j=0}^{\infty}}
\newcommand{\xbar}{\bar{X}}
\newcommand{\odds}[1]{\frac{#1}{1-#1}}
\newcommand{\ww}{\frac{\log t - z\beta}{\sigma}}
\newcommand{\wwm}{\frac{\log t_{0.5} - z\beta}{\sigma}}
\newcommand{\zz}[1]{\left[ {#1}\right] }
\newcommand{\zb}[1]{\left\lbrace  {#1}\right\rbrace  }
\newcommand{\nk}[2]{\left( \begin{array}{c} {#1}\\ {#2} \end{array}\right)}
\newcommand{\nuone}{\frac{\Gamma (\frac{\nu+1}{2})}{\Gamma (\frac{\nu}{2})}}
\newcommand{\nutwo}{\frac{\Gamma (\frac{\nu+2}{2})}{\Gamma (\frac{\nu}{2})}}
\newcommand{\bb}{\boldsymbol{\beta}}
\newcommand{\betaf}{\frac{\Gamma (\alpha+\beta)}{\Gamma (\alpha) \Gamma (\beta)}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\providecommand{\eeee}[1]{\ensuremath{\times 10^{#1}}}
\setstretch{1} %line spacing

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\pagestyle{empty}

%\pagestyle{plain}
%\newcommand{\hmwkTitle}{Assignment\ \#3} % Assignment title
%\newcommand{\hmwkDueDate}{Monday,\ March\ 24,\ 2014} % Due date
%\newcommand{\hmwkClass}{MATH\ 783} % Course/class
%\newcommand{\hmwkClassTime}{10:05am} % Class/lecture time
%\newcommand{\hmwkClassInstructor}{Dr. Abbas Khalili} % Teacher/lecturer
%\newcommand{\hmwkAuthorName}{Sahir Rai Bhatnagar} % Your name
%\newcommand{\hmwkStudentID}{id \# 260194225} % student id
%\newcommand{\heading}[1]{\bigskip \hrule \smallskip \noindent \texttt{#1} \smallskip \hrule}


\newcommand{\test}[2]{\heading{\textbf{Test }$\bs{H}_0$ : ${#1}$ \textbf{versus} $\bs{H}_A$ : ${#2}$ at $\alpha=0.05$}}
\newcommand{\tm}[1]{\textrm{{#1}}}
\newcommand{\xf}{\mathcal{X}}
\newcommand{\pfrac}[2]{\left( \frac{#1}{#2}\right)}
\newcommand{\e}{{\mathsf E}}
\newcommand{\bt}{\boldsymbol{\theta}}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}


\usepackage[pagebackref=true,bookmarks]{hyperref}
                             % Neat package to turn href, ref, cite, gloss entries
                             % into hyperlinks in the dvi file.
                             % Make sure this is the last package loaded.
                             % Use with dvips option to get hyperlinks to work in ps and pdf
                             % files.  Unfortunately, then they don't work in the dvi file!
                             % Use without the dvips option to get the links to work in the dvi file.

                             % Note:  \floatstyle{ruled} don't work properly; so change to plain.
                             % Not as pretty, but functional...
                             % The bookmarks option sets up proper bookmarks in the pdf file :)
\hypersetup{
    unicode=false,          
    pdftoolbar=true,        
    pdfmenubar=true,        
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={Assignment 2, MATH 680},    % title
    pdfauthor={Sahir Rai Bhatnagar},     % author
    pdfsubject={Subject},   % subject of the document
    pdfcreator={Sahir Rai Bhatnagar},   % creator of the document
    pdfproducer={Sahir Rai Bhatnagar}, % producer of the document
    pdfkeywords={}, % list of keywords
    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=blue,        % color of links to bibliography
    filecolor=black,      % color of file links
    urlcolor=cyan           % color of external links
}

 

%\title{
%\vspace{2in}
%\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
%\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at\ \hmwkClassTime}\\
%\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
%\vspace{3in}
%}

%\author{\textbf{\hmwkAuthorName}}

%\date{\hmwkStudentID} % Insert date here if you want it to appear below your name
%<<echo=FALSE, results= 'hide', message = FALSE>>=
%options(width=60)
%@
\newcommand{\bh}{\hat{\beta}}
\newcommand{\xtx}{\mathbf{X}^T\mathbf{X}}
\newcommand{\xtxinv}{\left(\mathbf{X}^T\mathbf{X}\right)^{-1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\dnorm}[3]{\frac{1}{\sqrt{2\pi #3}} \expp{- \frac{\left( #1-#2\right) ^2}{2 #3}}  }
\newcommand{\dpois}[3]{\frac{\exp\left(-#2\right) #3 }{#1 !}}

\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.0pt}

\setlength{\textheight}{9.00in}
\setlength{\textwidth}{7.00in}
\setlength{\topmargin}{-0.5in}
\setlength{\evensidemargin}{-0.25in}
\setlength{\oddsidemargin}{-0.25in}

\renewcommand{\baselinestretch}{1.2}

\makeatletter

\makeatother
\lfoot{} \cfoot{ } \rfoot{{\small{\em Page \thepage \ of \pageref{LastPage}}}}
\date{}
\begin{document}
\pagestyle{fancy}

%\title{\vspace{-7ex}MATH 680 - Assignment 1 - February 20, 2015}
%\author{Sahir Bhatnagar \vspace{-4ex}}

\title{MATH 680 - Assignment 2 - March 30, 2015}
\author{Sahir Bhatnagar \& Maxime Turgeon}

\affil{Department of Epidemiology, Biostatistics and Occupational Health \\ McGill University}
%\date{}
\maketitle
\thispagestyle{empty}

M~Turgeon reproduced Figures~\ref{fig:figure-1},~\ref{fig:figure-2}, and~\ref{fig:figure-5} as well as Table~\ref{tab:table21}. S~Bhatnagar reproduced Tables~\ref{tab:table1},~\ref{tab:table2}, and~\ref{tab:table-3} as well as Figure~\ref{fig:figure-3}.

M~Turgeon summarized Sections 5, 6, and 7, as well as Gelman's \& Vehtari's comment and the Rejoinder. S~Bhatnagar summarized the other comments.

\clearpage
\setcounter{page}{1}

%\begin{abstract}
%Here is our abstract.  We have taken the ideas from \citet{WuLange2008}, and coded them up.
%\end{abstract}

\section{Numerical Results}

<<packages,echo=FALSE,warning=FALSE,message=FALSE>>=
#setwd("~/git_repositories/math680/")
pkgTest <- function(x)
{
    if (!require(x,character.only = TRUE))
    {
        install.packages(x,dep=TRUE)
        if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}
pkgTest("knitr")
pkgTest("plyr")
pkgTest("data.table")
pkgTest("doParallel")
pkgTest("foreach")
pkgTest("bootstrap")
pkgTest("splines")
pkgTest("ggplot2")
pkgTest("reshape2")
pkgTest("car")
pkgTest("grid")
pkgTest("calibrate")
pkgTest("xtable")
pkgTest("leaps")
registerDoParallel(cores = 4)
source("multiplot.R")
source("functions.R")
knit_hooks$set(crop = hook_pdfcrop)
knit_hooks$set(purl = hook_purl)
@


<<setup, echo=FALSE,warning=FALSE,message=FALSE,cache=FALSE>>=
options(width=70, digits=2)
set.seed(45)
opts_chunk$set(echo = FALSE, tidy = TRUE, cache = FALSE)
opts_template$set(
    fig.large = list(fig.width = 7, fig.height = 5, fig.align = 'center'),
    fig.small = list(fig.width = 3.5, fig.height = 3, fig.align = 'center')
)
read_chunk("section1.R")
read_chunk("section2.R")
read_chunk("section3.R")
read_chunk("section4.R")
read_chunk("discussion_sy.R")
#knitr:::knit_code$get()
@

We attempted to reproduce the numerical computations that were performed in the first four sections of Efron's 2014 JASA article. 

\subsection{Introduction}

\FloatBarrier

<<import-data, echo=FALSE,warning=FALSE,message=FALSE>>=
@

<<hist-compliance,echo=FALSE,fig.cap='Histogram of compliance values after transformation, so that it looks like a standard normal distribution',fig.align='center',opts.label = 'fig.large', crop = TRUE, eval=FALSE>>=
@

<<figure-1,echo=FALSE,fig.cap='Choleterol decrease vs. compliance with cubic regression fitted curve',fig.align='center',opts.label = 'fig.large', crop = TRUE>>=
@

The compliance measures were transformed to make them closer to a standard normal random variables. It involved a rank transformation. Using these values, we fitted a cubic regression of compliance on cholesterol decrease. In Figure~\ref{fig:figure-1}, we have a scatterplot of the data, and the regression curve has been added to it.

\subsection{Nonparametric Bootstrap Smoothing}

<<bootstrap-samples>>=
@

<<Cp-boot, cache=2>>=
@

<<Cp-original>>=
@

<<table1, results='asis',echo=FALSE>>=
print(xtable(tab1,digits=0,label="tab:table1",caption = "$C_p$ model selection for the Cholesterol data. $\\sigma=22$ was used in all bootstrap replications. Last column shows percentage each model was selected as the $C_p$ minimizer, among $B = 4000$ bootstrap replications",align = c("l","l","c","c","c")),include.rownames=FALSE)
@


<<table2, results='asis',echo=FALSE>>=
print(xtable(tab2, caption = "Mean and standard deviation of $\\hat{\\mu}_1^*$ as a function of the selected model, 4000 nonparametric boostrap replications.", label="tab:table2"))
@

<<table21, results='asis',echo=FALSE,warning=FALSE,message=FALSE>>=
print(xtable(tab21, caption = "Mean and standard deviation of $\\hat{\\mu}_1^*$ as a function of the selected model. Some bootstrap samples that led to the quintic and sextic model being selected give a bad fit. Removing them and recomputing table 2 shows that the discrepancy between our table 2 and Efron's is due to bootstrap variability.", label="tab:table21"))
@

In Table~\ref{tab:table1}, we have computed the $C_p$ statistic for each model, showing that the cubic model indeed provides the lowest value. We also performed model selection for each of $B=4000$ bootstrap resamples, and recorded which model was selected. The last column shows what proportion of resamples led to a given model being selected.

In Table~\ref{tab:table2}, we have the mean and standard deviation of the bootstrap estimates, as a function of the selected model. Our values for the quintic and sextic models are wuite far from Efron's results. This is due to some bootstrap samples leading to especially bad fits. When removing some samples that where quite far from the overall mean, we can more or less recreate Efron's results (see Table~\ref{tab:table21}). In any case, we remark that different models lead to very different values of the bootstrap estimates. This is also apparent in Figure~\ref{fig:discussion-1}, where we have plotted the histogram of the bootstrap estimates corresponding to the three models being selected the most often: linear, cubic, and quartic. The discrete nature of our search through the collection of models creates these separate histograms, and therefore leads to the ``jumpiness'' nature of the estimator. The overall histogram of the bootstrap estimates appears in Figure~\ref{fig:figure-3}.

<<figure-2,echo=FALSE,fig.cap='Blue points: ratio of standard deviations, taking account of model selection or not, for the 164 values $\\hat{\\mu}_j$ from the regression curve in Figure 1. Red points: ratio of $\\tilde{sd}_B$ to $\\hat{sd}_B$',fig.align='center',opts.label = 'fig.large', crop = TRUE>>=
@


<<figure-3,echo=FALSE,fig.cap='B=4000 bootstrap replications of the Cp-OLS regression estimate for Subject 1. Triangles indicate 2.5th and 97.5th percentiles of the histogram',fig.align='center',opts.label = 'fig.large', crop = TRUE>>=
@

<<discussion-1,echo=FALSE,fig.cap='Fitted values for subject 1, from B=4000 nonparametric bootsrap replications separated by three most frequently chosen models by Cp',fig.align='center',opts.label = 'fig.large', crop = TRUE, warning=FALSE, message=FALSE>>=
@



\subsection{Accuracy of the Smoothed Bootstrap Estimates}

<<table-3,echo=FALSE,warning=FALSE, cache=2>>=
@



<<results='asis', echo=FALSE>>=
print(xtable(p.conf,digits = 2,label="tab:table-3", caption = 'Three approximate 95\\% bootstrap confidence intervals 
             for $\\mu_1$, the response for Subject 1, Cholesterol data'),include.rownames=FALSE)
@

Using Theorem~1, we can now compute the standard deviation of the smoothed bootstrap estimator and therefore construct confidence intervals. The comparison of three different construction is given in Table~\ref{tab:table-3}. In Figure~\ref{fig:figure-2}, we have a comparison of the smoothed vs. non-smoothed standard deviations (red dots), as well as a comparison of the smoothed standard deviation and the one coming from assuming that the cubic model is the correct one (blue dots). Our results here are different from Efron's: the median of the blue dots is \Sexpr{median(JJ$sdtilde/JJ$sdhat)}, but the median of the red dots is only \Sexpr{median(JJ$sdtilde/SEbar)}. 

\subsection{Parametric Bootstrap Smoothing}

<<figure-5,echo=FALSE,fig.cap='Simulation test of Theorem 2. Cholesterol data; 100 simulations, 1000 parametric bootstraps each, for the 11 subjects indicated at the bottom of Figure 1',fig.align='center',opts.label = 'fig.large', crop = TRUE, cache=2>>=
@

In Section~4, Efron explains how the smoothed bootstrap can be extended to the parametric setting. The main results is Theorem~2, in which he derives the standard deviation of the estimator. Figure~\ref{fig:figure-5} shows the results of a simulation test that tries to recreate the results of Theorem~2, by comparing the proposed expression for the standard deviation to its empirical counterpart. Again, our results are slightly different from Efron's: the overall shape of the curve is reproduced, but the $y$-scale is different, with smaller standard deviations than Efron.

\subsection{Section 5, 6, and 7}

Section 5 discusses another example of parametric bootstrap. Efron uses a different dataset: the response is the absolute magnitude for 39 Type 1a supernovae, along with their spectral energies measured at 10 different frequencies. 

Section 6 looks at improving the bootstrap confidence intervals: the idea is to increase the convergence rate of the coverage probabilities from $\sqrt{n}$ to $n$, where $n$ is the sample size. Efron proposes to do this using the ABC system (proposed by DiCiccio and Efron), which corrects the smoothed confidence interval on three levels: (1) non-normality correction, (2) acceleration correction, and (3) bias correction. This methodology can be used in the context of an ``empirical exponential family''.

Finally, section 7 is a collection of various proofs and remarks about the previous sections. Of particular interest is a discussion on how to estimate the bias of the smoother estimates. 

\section{Discussion}

The main example in this paper was based on all subset selection in a linear regression context. The discussion by Wang, Sherwood and Li investigates the proposed method in a regularization procedure, a GLM, quantile and nonparametric regression. Overall, their numerical examples show that Efron's proposed smoothed estimator results in more accurate confidence intervals with good coverage probabilities. The rationale behind using $L_1$-norm type penalty functions for model selection as opposed to the all subset method used by Efron, is due to the well known result that all subset selection methods are unstable~\citep{breiman1996}. Since these model selection procedures are driven by the data, it has been shown that even changing one point from the learning set, can result in a completely different chosen model~\citep{breimanmachine}.

We performed a similar analysis on the \texttt{prostate} data set~\citep{prostate}; a study that examined the relation between level of prostate specific antigen (PSA) and eight clinical measures (e.g. Gleason score, cancer volume, prostate weight) in 97 men who were about to receive prostatectomy. We produced $B=4000$ bootstrap samples of the \texttt{prostate} data set. For each of the bootstrap samples, we performed model selection using all subset selection as well as LASSO~\citep{Tibshirani94regressionshrinkage}, SCAD~\citep{fanli} and MCP~\citep{zhang2010}. For each model selection procedure, we calculated the fitted value for observation 95 based on the chosen model (resulting in 4000 fitted values for each of the 5 procedures). All the analysis was performed in \texttt{R}~\citep{rcore}. The LASSO was implemented using the \texttt{glmnet} package~\citep{friedmanjstat}. SCAD and MCP were implemented using the coordinate descent algorithm in the \texttt{ncvreg} package~\citep{breheny}. The tuning parameter $\lambda$ was chosen using 5 fold cross validation. Note, for the SCAD and MCP penalties, which have an additional tuning parameter, we used the suggested value of $3.7$ and $3.0$, respectively. All subset selection via the BIC and Cp criterion were implemented using the \texttt{leaps} package~\citep{leaps}. The histograms for the fitted values are given in Figure~\ref{fig:proshist}.   

\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./mcp_scad_las_hist}
                \caption{$L_1$-Norm Penalty Functions}
                \label{fig:proshist1}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./bic_cp_hist}
                \caption{All Subset Selection}
                \label{fig:proshist2}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        %\begin{subfigure}[b]{0.3\textwidth}
        %        \centering
        %        \includegraphics[width=\textwidth]{mouse}
        %        \caption{A mouse}
        %        \label{fig:mouse}
        %\end{subfigure}
        \caption{Histogram of fitted values for subject 95 based on 4000 Bootstrap samples for the \texttt{prostate} data set}\label{fig:proshist}
\end{figure}
\FloatBarrier
Figure~\ref{fig:proshist2} confirms the work of~\cite{breiman1996}, i.e., the all subset selection procedure is very sensitive to changes in the data. The LASSO, SCAD and MCP all produce much more stable estimates, with the distributions of the fitted values looking normal and centred around their mean (Figure~\ref{fig:proshist1}).\\

Figure~\ref{fig:prosconf} compares the lengths of the three confidence intervals types for each procedure. We see that the new smoothed confidence intervals, based on the proposed smoothed standard deviation $\widehat{sd}_B$, outperforms (i.e. is shorter than) the standard and quantile confidence intervals across all model selection procedures, with the LASSO providing the smallest intervals. Table~\ref{tab:prostab} provides the numerical values of our analysis. Again, we see that the $L_1$-Norm penalties all perform similarly, with good coverage probabilities. Although the coverage probabilities are reasonable for the all subset methods, the length of the interval is much wider. 

\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./ci_penalty}
                \caption{$L_1$-Norm Penalty Functions}
                \label{fig:prosconf1}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./ci_penalty_bic}
                \caption{All Subset Selection}
                \label{fig:prosconf2}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        %\begin{subfigure}[b]{0.3\textwidth}
        %        \centering
        %        \includegraphics[width=\textwidth]{mouse}
        %        \caption{A mouse}
        %        \label{fig:mouse}
        %\end{subfigure}
        \caption{Length of confidence intervals for $\hat{\mu}_{95}$ based on 4000 Bootstrap samples for the \texttt{prostate} data set}\label{fig:prosconf}
\end{figure}



\ctable[caption={Prostate data, B=4000, Observation 95},label=tab:prostab,pos=h!, doinside=\normalsize]{lcccccccc}{}
{
															\FL
model & type & fitted value & sd & length & coverage \ML
LASSO & standard & 3.62 & 0.31 & 1.21 & 0.94 \\ 
   & quantile &  &  & 1.20 & 0.95 \\ 
   & smooth & 3.57 & 0.29 & 1.14 & 0.93 \ML
  SCAD & standard & 3.60 & 0.35 & 1.37 & 0.95 \\ 
   & quantile &  &  & 1.33 & 0.95 \\ 
   & smooth & 3.62 & 0.33 & 1.28 & 0.93 \ML
  MCP & standard & 3.60 & 0.35 & 1.38 & 0.96 \\ 
   & quantile &  &  & 1.35 & 0.95 \\ 
   & smooth & 3.61 & 0.33 & 1.29 & 0.94 \ML
  BIC & standard & 5.50 & 4.75 & 18.62 & 0.84 \\ 
   & quantile &  &  & 16.05 & 0.95 \\ 
   & smooth & 3.22 & 3.46 & 13.55 & 0.83 \ML 
  Cp & standard & 5.13 & 5.11 & 20.02 & 0.86 \\ 
   & quantile &  &  & 16.15 & 0.95 \\ 
   & smooth & 0.64 & 4.40 & 17.24 & 0.97 \LL
}

Wang, Sherwood and Li also show that when the columns of the predictor matrix are orthogonal and $C_p$, AIC or BIC are used as model selection criteria, an analytical solution to the asymptotic variance of the smoothed estimator can be derived . With a numerical example, they show that Efron's estimator performs well in this setting. Professor Politis points out that 
the proposed estimator does not apply to the residual bootstrap because this presupposes a choice of the model. He then gives a summary of his own work on model-free prediction as an alternative approach. Gupta and Lahiri present two alternative methods for constructing confidence intervals: the Adaptive LASSO~\citep{zou} and maximum frequency Bootstrap-$t$ (MF). The Adaptive LASSO has the oracle property, i.e., it performs asymptotically as well as if the true underlying model were given in advance. The maximum frequency Bootstrap-$t$ limits the calculation of the bagged estimator and its standard error to those resamples that led to the most chosen model. While Gupta and Lahiri's simulations show good performance of this approach, it does not do well in the cholesterol example;  MF 95\% CI [-5.93,15.35] compared to the smoothed interval [-13.3, 8.0]. This is because a substantial number of bootstrap resamples, which led to more negative predicted values (e.g. Table~\ref{tab:table1}), are being ignored by the MF.  

Gelman and Vehtari offer a word of caution about the apparent generality of the bootstrap methodology. They give an example where it works well: when the models under consideration are relatively large, or when the dataset itself is large, posterior simulation in a Bayesian context can be costly; effective sampling of the parameter space can be difficult. They suggest that bootstrap smoothing can be helpful in constructing posterior intervals with small mean squared error. They then given an example were the bootstrap does not work well: in logistic regression, when a predictor configuration leads to only to events (or no event) being observed (i.e. $y_i=0$ or $y_i=1$ for all observations under that configuration), maximum likelihood estimates of the regression parameter will be unbounded, and smoothing will not be able to help. Moreover, bootstrap can exacerbate this behaviour: resampling with replacement can create synthetic datasets which exhibit this behaviour. Gelman and Vehtari thus suggest using regularization techniques \emph{before} using the bagging approach.

Finally, in the Rejoinder, Efron thanks the discussants for expanding on his idea. He praises particularly the discussion by Wang, Sherwood and Li, which gives support to his approach by investigating its generality. However, he seems quite skeptical about its applicability in a high-dimensional setting. Note that he also mentions that bagging can be thought of as a form of nonparametric maximum likelihood estimation.


\newpage

\FloatBarrier
\bibliography{report}



\end{document}